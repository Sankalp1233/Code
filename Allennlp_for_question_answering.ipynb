{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Allennlp for question answering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMadNnFw8sT1Had/XzQdGgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sankalp1233/Code/blob/main/Allennlp_for_question_answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdgDEhBbg9m5",
        "outputId": "e509524f-8d72-4cea-f055-88391876f768"
      },
      "source": [
        "!pip3 install allennlp\n",
        "# !pip3 install urllib3==1.25.4\n",
        "# !pip3 install folium==0.2.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allennlp\n",
            "  Using cached https://files.pythonhosted.org/packages/ed/eb/0c7ce6af0dfbc9c168a263a534e258447eff707c76df26356d4080900dd0/allennlp-2.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.17.57)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.95)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2)\n",
            "Requirement already satisfied: wandb<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.17.0)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.8.1+cu101)\n",
            "Requirement already satisfied: transformers<4.6,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.5.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp) (3.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp) (1.15.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp) (1.20.57)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (56.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (1.25.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.8.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (2.3)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.5.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (5.0.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (1.0.1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp) (3.1.14)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp) (0.10.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub>=0.0.8->allennlp) (3.4.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6,>=4.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp) (4.0.0)\n",
            "Installing collected packages: allennlp\n",
            "Successfully installed allennlp-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "7beAnliTlUwz",
        "outputId": "290ed792-7ca1-47f7-d17f-db97f7e0eca7"
      },
      "source": [
        "from allennlp import pretrained\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-361842327bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'pretrained' from 'allennlp' (/usr/local/lib/python3.7/dist-packages/allennlp/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5FRKOH_lzZj",
        "outputId": "37347f10-9c1d-4cc2-f706-272ea31c5712"
      },
      "source": [
        "# !pip3 install git+https://github.com/allenai/allennlp.git@main"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/allenai/allennlp-models\n",
            "  Cloning https://github.com/allenai/allennlp-models to /tmp/pip-req-build-t96uc8vf\n",
            "  Running command git clone -q https://github.com/allenai/allennlp-models /tmp/pip-req-build-t96uc8vf\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): allennlp-models==2.4.0 from git+https://github.com/allenai/allennlp-models in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (3.2.5)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (6.0.1)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (1.1)\n",
            "Requirement already satisfied: torch<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: conllu==4.4 in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (4.4)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (1.1)\n",
            "Requirement already satisfied: allennlp@ git+https://github.com/allenai/allennlp.git@main from git+https://github.com/allenai/allennlp.git@main in /usr/local/lib/python3.7/dist-packages (from allennlp-models==2.4.0) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp-models==2.4.0) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp-models==2.4.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.7.0->allennlp-models==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<1.9.0,>=1.7.0->allennlp-models==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: wandb<0.11.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.10.27)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.4.1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.0.12)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (8.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.1.95)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.10.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.14 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.17.57)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.8 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.0.8)\n",
            "Requirement already satisfied: transformers<4.6,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (4.5.1)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.2)\n",
            "Requirement already satisfied: overrides==3.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.1.0)\n",
            "Requirement already satisfied: torchvision<0.10.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.9.1+cu101)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (4.41.1)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.17.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.6.4)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.99)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.22.2.post1)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.1.14)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (5.0.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.8.1)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.0.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (5.4.8)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.5.4)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (7.1.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (56.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.8.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.20.57)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.14->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.4.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.8->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.10.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6,>=4.1->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.0.45)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.10.0,>=0.8.1->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (7.1.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (20.3.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.25.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (4.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub>=0.0.8->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6,>=4.1->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (2.4.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.11.0,>=0.10.0->allennlp@ git+https://github.com/allenai/allennlp.git@main->allennlp-models==2.4.0) (4.0.0)\n",
            "Building wheels for collected packages: allennlp-models\n",
            "  Building wheel for allennlp-models (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for allennlp-models: filename=allennlp_models-2.4.0-cp37-none-any.whl size=425884 sha256=6ef84769d2d754b1259b9ddc1b57e7b875e3e1cd9240b2452c0019fbc1088b6f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9u6tk5op/wheels/e1/74/40/b9b56d030aa61c0368d97d0acffd9963e4b89939a4c9a671fd\n",
            "Successfully built allennlp-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMlG9U3tmVXC",
        "outputId": "ee66311f-efce-43ea-c0ac-52a8bb93573a"
      },
      "source": [
        "!pip3 install streamlit\n",
        "!pip3 install seaborn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/99/a8913c21bd07a14f72658a01784414ffecb380ddd0f9a127257314fea697/streamlit-0.80.0-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 15.1MB/s \n",
            "\u001b[?25hCollecting blinker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 34.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Collecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Collecting watchdog; platform_system != \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b2/b4ebe23174fd00ec94ac3f58ebf85f1090c49858feab1ca62ed7ea4d2f2f/watchdog-2.0.3-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (56.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.1.2)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (1.1.1)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=4325e9c52e2ec634750b8afa37b44f7a53540d4fb0981b98d71417bb946e9680\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built blinker\n",
            "Installing collected packages: blinker, base58, watchdog, validators, smmap, gitdb, gitpython, pydeck, streamlit\n",
            "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.14 pydeck-0.6.2 smmap-4.0.0 streamlit-0.80.0 validators-0.18.2 watchdog-2.0.3\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.1.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SfevackmNGy",
        "outputId": "1160328c-eb4d-42a9-9f84-1616ececeed8"
      },
      "source": [
        "# !pip3 uninstall ipykernel\n",
        "!pip3 install ipykernel==5.1.2\n",
        "import streamlit as st\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipykernel==5.1.2 in /usr/local/lib/python3.7/dist-packages (5.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel==5.1.2) (5.3.5)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel==5.1.2) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel==5.1.2) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel==5.1.2) (5.0.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel==5.1.2) (22.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel==5.1.2) (2.8.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel==5.1.2) (4.7.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (56.0.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel==5.1.2) (0.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.1.0->ipykernel==5.1.2) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel==5.1.2) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel==5.1.2) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel==5.1.2) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKhVw7QIo_Zl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bzjpRlXna3U",
        "outputId": "3055ffe4-80ae-4a95-978c-c2069db8a120"
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install git+https://github.com/allenai/allennlp-hub\n",
        "st.title(\"Question Answering\")\n",
        "import pytest\n",
        "import spacy\n",
        "from allennlp.common.testing import AllenNlpTestCase\n",
        "from allennlp_hub import pretrained\n",
        "model = st.cache(\n",
        "       pretrained.bidirectional_attention_flow_seo_2017,\n",
        "       allow_output_mutation=True\n",
        ")()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Collecting git+https://github.com/allenai/allennlp-hub\n",
            "  Cloning https://github.com/allenai/allennlp-hub to /tmp/pip-req-build-jg9dccgu\n",
            "  Running command git clone -q https://github.com/allenai/allennlp-hub /tmp/pip-req-build-jg9dccgu\n",
            "Requirement already satisfied (use --upgrade to upgrade): allennlp-hub===0.0.1-unreleased from git+https://github.com/allenai/allennlp-hub in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp from git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp in /usr/local/lib/python3.7/dist-packages (from allennlp-hub===0.0.1-unreleased) (0.9.1-unreleased)\n",
            "Requirement already satisfied: allennlp_semparse@ git+https://github.com/allenai/allennlp-semparse@339e617861a7616618a503cc98e1e9c8b28a1b06#egg=allennlp-semparse from git+https://github.com/allenai/allennlp-semparse@339e617861a7616618a503cc98e1e9c8b28a1b06#egg=allennlp-semparse in /usr/local/lib/python3.7/dist-packages (from allennlp-hub===0.0.1-unreleased) (0.0.1-unreleased)\n",
            "Requirement already satisfied: allennlp_reading_comprehension@ git+https://github.com/allenai/allennlp-reading-comprehension@e4f8e5df4f9fa35287d44e94fc8b26b9cabed0a5#egg=allennlp-reading-comprehension from git+https://github.com/allenai/allennlp-reading-comprehension@e4f8e5df4f9fa35287d44e94fc8b26b9cabed0a5#egg=allennlp-reading-comprehension in /usr/local/lib/python3.7/dist-packages (from allennlp-hub===0.0.1-unreleased) (0.0.1-unreleased)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (4.41.1)\n",
            "Requirement already satisfied: overrides==2.8.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.8.0)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.7.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.2.5)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.2)\n",
            "Requirement already satisfied: conllu==2.2.2 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.23.0)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.8.5)\n",
            "Requirement already satisfied: torch!=1.3.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.10.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.0.0)\n",
            "Requirement already satisfied: spacy<2.3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.22.2.post1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.6.4)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.13.2)\n",
            "Requirement already satisfied: transformers<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.4.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.17.57)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from allennlp_semparse@ git+https://github.com/allenai/allennlp-semparse@339e617861a7616618a503cc98e1e9c8b28a1b06#egg=allennlp-semparse->allennlp-hub===0.0.1-unreleased) (1.2.0)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp_semparse@ git+https://github.com/allenai/allennlp-semparse@339e617861a7616618a503cc98e1e9c8b28a1b06#egg=allennlp-semparse->allennlp-hub===0.0.1-unreleased) (0.4.1)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from allennlp_semparse@ git+https://github.com/allenai/allennlp-semparse@339e617861a7616618a503cc98e1e9c8b28a1b06#egg=allennlp-semparse->allennlp-hub===0.0.1-unreleased) (0.8.1)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp_semparse@ git+https://github.com/allenai/allennlp-semparse@339e617861a7616618a503cc98e1e9c8b28a1b06#egg=allennlp-semparse->allennlp-hub===0.0.1-unreleased) (0.5.3)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp_reading_comprehension@ git+https://github.com/allenai/allennlp-reading-comprehension@e4f8e5df4f9fa35287d44e94fc8b26b9cabed0a5#egg=allennlp-reading-comprehension->allennlp-hub===0.0.1-unreleased) (1.1)\n",
            "Requirement already satisfied: py-rouge==1.1 in /usr/local/lib/python3.7/dist-packages (from allennlp_reading_comprehension@ git+https://github.com/allenai/allennlp-reading-comprehension@e4f8e5df4f9fa35287d44e94fc8b26b9cabed0a5#egg=allennlp-reading-comprehension->allennlp-hub===0.0.1-unreleased) (1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch!=1.3.0,>=1.2.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.10.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (56.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.3,>=2.1.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.0.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (20.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.7.1)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.7/dist-packages (from transformers<2.5.0,>=2.4.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.0.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers<2.5.0,>=2.4.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.1.95)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<2.5.0,>=2.4.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<2.5.0,>=2.4.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<2.5.0,>=2.4.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2019.12.20)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (1.20.57)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (0.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<2.5.0,>=2.4.0->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.57->boto3->allennlp@ git+https://github.com/allenai/allennlp@65ff0d87a30e6532cb21ea5fe8b7bd436445c128#egg=allennlp->allennlp-hub===0.0.1-unreleased) (2.8.1)\n",
            "Building wheels for collected packages: allennlp-hub\n",
            "  Building wheel for allennlp-hub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for allennlp-hub: filename=allennlp_hub-0.0.1_unreleased-cp37-none-any.whl size=9304 sha256=8ab1db89aec80a4621e4de1680c6b586206ec12461748e849b97693397b51f7b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ep9mng8f/wheels/3f/98/4b/10be33109d4ccb88c491fb47cb827300bbfb9faf9fa92e95bc\n",
            "Successfully built allennlp-hub\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-26 19:53:47.001 WARNING root: \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "2021-04-26 19:53:50.456 INFO    transformers.file_utils: PyTorch version 1.8.1+cu101 available.\n",
            "2021-04-26 19:53:52.090 INFO    transformers.file_utils: TensorFlow version 2.4.1 available.\n",
            "2021-04-26 19:53:53.657 INFO    allennlp.common.file_utils: https://allennlp.s3.amazonaws.com/models/bidaf-model-2020.02.10-charpad.tar.gz not found in cache, downloading to /tmp/tmp8y03qd9z\n",
            "100%|██████████| 46219049/46219049 [00:02<00:00, 22114817.38B/s]\n",
            "2021-04-26 19:53:56.222 INFO    allennlp.common.file_utils: copying /tmp/tmp8y03qd9z to cache at /root/.allennlp/cache/a57a14f7f9bda99109bda9e2f0d06d6ab0db27ac0b8e5dfc447b63de05ec89cd.f113e528c390f2d391913b50c87f28d96127d4f1a5024107221c3d97b8e03e43\n",
            "2021-04-26 19:53:56.297 INFO    allennlp.common.file_utils: creating metadata file for /root/.allennlp/cache/a57a14f7f9bda99109bda9e2f0d06d6ab0db27ac0b8e5dfc447b63de05ec89cd.f113e528c390f2d391913b50c87f28d96127d4f1a5024107221c3d97b8e03e43\n",
            "2021-04-26 19:53:56.302 INFO    allennlp.common.file_utils: removing temp file /tmp/tmp8y03qd9z\n",
            "2021-04-26 19:53:56.313 INFO    allennlp.models.archival: loading archive file https://allennlp.s3.amazonaws.com/models/bidaf-model-2020.02.10-charpad.tar.gz from cache at /root/.allennlp/cache/a57a14f7f9bda99109bda9e2f0d06d6ab0db27ac0b8e5dfc447b63de05ec89cd.f113e528c390f2d391913b50c87f28d96127d4f1a5024107221c3d97b8e03e43\n",
            "2021-04-26 19:53:56.316 INFO    allennlp.models.archival: extracting archive file /root/.allennlp/cache/a57a14f7f9bda99109bda9e2f0d06d6ab0db27ac0b8e5dfc447b63de05ec89cd.f113e528c390f2d391913b50c87f28d96127d4f1a5024107221c3d97b8e03e43 to temp dir /tmp/tmpwaeh9pzr\n",
            "2021-04-26 19:53:56.861 INFO    allennlp.common.params: type = from_instances\n",
            "2021-04-26 19:53:56.863 INFO    allennlp.data.vocabulary: Loading token dictionary from /tmp/tmpwaeh9pzr/vocabulary.\n",
            "2021-04-26 19:53:56.976 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.models.model.Model'> from params {'dropout': 0.2, 'modeling_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 800, 'num_layers': 2, 'type': 'lstm'}, 'num_highway_layers': 2, 'phrase_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 200, 'num_layers': 1, 'type': 'lstm'}, 'similarity_function': {'combination': 'x,y,x*y', 'tensor_1_dim': 200, 'tensor_2_dim': 200, 'type': 'linear'}, 'span_end_encoder': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 1400, 'num_layers': 1, 'type': 'lstm'}, 'text_field_embedder': {'token_embedders': {'token_characters': {'dropout': 0.2, 'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 100, 'trainable': False, 'type': 'embedding'}}}, 'type': 'bidaf'} and extras {'vocab'}\n",
            "2021-04-26 19:53:56.978 INFO    allennlp.common.params: model.type = bidaf\n",
            "2021-04-26 19:53:56.981 INFO    allennlp.common.from_params: instantiating class <class 'allennlp_rc.models.bidaf.BidirectionalAttentionFlow'> from params {'dropout': 0.2, 'modeling_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 800, 'num_layers': 2, 'type': 'lstm'}, 'num_highway_layers': 2, 'phrase_layer': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 200, 'num_layers': 1, 'type': 'lstm'}, 'similarity_function': {'combination': 'x,y,x*y', 'tensor_1_dim': 200, 'tensor_2_dim': 200, 'type': 'linear'}, 'span_end_encoder': {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 1400, 'num_layers': 1, 'type': 'lstm'}, 'text_field_embedder': {'token_embedders': {'token_characters': {'dropout': 0.2, 'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 100, 'trainable': False, 'type': 'embedding'}}}} and extras {'vocab'}\n",
            "2021-04-26 19:53:56.985 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'token_characters': {'dropout': 0.2, 'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 100, 'trainable': False, 'type': 'embedding'}}} and extras {'vocab'}\n",
            "2021-04-26 19:53:56.989 INFO    allennlp.common.params: model.text_field_embedder.type = basic\n",
            "2021-04-26 19:53:56.992 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder'> from params {'token_embedders': {'token_characters': {'dropout': 0.2, 'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 100, 'trainable': False, 'type': 'embedding'}}} and extras {'vocab'}\n",
            "2021-04-26 19:53:56.996 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0.2, 'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab'}\n",
            "2021-04-26 19:53:56.997 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.type = character_encoding\n",
            "2021-04-26 19:53:57.001 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.token_embedders.token_characters_encoder.TokenCharactersEncoder'> from params {'dropout': 0.2, 'embedding': {'embedding_dim': 16, 'num_embeddings': 262}, 'encoder': {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'}} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.004 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.token_embedders.embedding.Embedding'> from params {'embedding_dim': 16, 'num_embeddings': 262} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.006 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim = 16\n",
            "2021-04-26 19:53:57.007 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.num_embeddings = 262\n",
            "2021-04-26 19:53:57.010 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.projection_dim = None\n",
            "2021-04-26 19:53:57.011 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.weight = None\n",
            "2021-04-26 19:53:57.013 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.padding_index = None\n",
            "2021-04-26 19:53:57.016 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.trainable = True\n",
            "2021-04-26 19:53:57.018 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.max_norm = None\n",
            "2021-04-26 19:53:57.019 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.norm_type = 2.0\n",
            "2021-04-26 19:53:57.020 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.scale_grad_by_freq = False\n",
            "2021-04-26 19:53:57.022 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.sparse = False\n",
            "2021-04-26 19:53:57.023 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.vocab_namespace = tokens\n",
            "2021-04-26 19:53:57.025 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.embedding.pretrained_file = None\n",
            "2021-04-26 19:53:57.065 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100, 'type': 'cnn'} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.067 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.encoder.type = cnn\n",
            "2021-04-26 19:53:57.069 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 16, 'ngram_filter_sizes': [5], 'num_filters': 100} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.070 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim = 16\n",
            "2021-04-26 19:53:57.072 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.encoder.num_filters = 100\n",
            "2021-04-26 19:53:57.073 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes = [5]\n",
            "2021-04-26 19:53:57.074 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.encoder.conv_layer_activation = None\n",
            "2021-04-26 19:53:57.076 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.encoder.output_dim = None\n",
            "2021-04-26 19:53:57.095 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.token_characters.dropout = 0.2\n",
            "2021-04-26 19:53:57.096 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 100, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.097 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.type = embedding\n",
            "2021-04-26 19:53:57.099 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.token_embedders.embedding.Embedding'> from params {'embedding_dim': 100, 'trainable': False} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.100 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.embedding_dim = 100\n",
            "2021-04-26 19:53:57.102 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
            "2021-04-26 19:53:57.103 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
            "2021-04-26 19:53:57.104 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.weight = None\n",
            "2021-04-26 19:53:57.105 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.padding_index = None\n",
            "2021-04-26 19:53:57.106 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.trainable = False\n",
            "2021-04-26 19:53:57.107 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
            "2021-04-26 19:53:57.109 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
            "2021-04-26 19:53:57.110 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
            "2021-04-26 19:53:57.111 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.sparse = False\n",
            "2021-04-26 19:53:57.112 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
            "2021-04-26 19:53:57.113 INFO    allennlp.common.params: model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
            "2021-04-26 19:53:57.273 INFO    allennlp.common.params: model.num_highway_layers = 2\n",
            "2021-04-26 19:53:57.276 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 200, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.278 INFO    allennlp.common.params: model.phrase_layer.type = lstm\n",
            "2021-04-26 19:53:57.281 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper.LstmSeq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 200, 'num_layers': 1} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.283 INFO    allennlp.common.params: model.phrase_layer.input_size = 200\n",
            "2021-04-26 19:53:57.284 INFO    allennlp.common.params: model.phrase_layer.hidden_size = 100\n",
            "2021-04-26 19:53:57.285 INFO    allennlp.common.params: model.phrase_layer.num_layers = 1\n",
            "2021-04-26 19:53:57.286 INFO    allennlp.common.params: model.phrase_layer.bias = True\n",
            "2021-04-26 19:53:57.288 INFO    allennlp.common.params: model.phrase_layer.dropout = 0.2\n",
            "2021-04-26 19:53:57.289 INFO    allennlp.common.params: model.phrase_layer.bidirectional = True\n",
            "2021-04-26 19:53:57.290 INFO    allennlp.common.params: model.phrase_layer.stateful = False\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "2021-04-26 19:53:57.299 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.similarity_functions.similarity_function.SimilarityFunction'> from params {'combination': 'x,y,x*y', 'tensor_1_dim': 200, 'tensor_2_dim': 200, 'type': 'linear'} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.301 INFO    allennlp.common.params: model.similarity_function.type = linear\n",
            "2021-04-26 19:53:57.302 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.similarity_functions.linear.LinearSimilarity'> from params {'combination': 'x,y,x*y', 'tensor_1_dim': 200, 'tensor_2_dim': 200} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.304 INFO    allennlp.common.params: model.similarity_function.tensor_1_dim = 200\n",
            "2021-04-26 19:53:57.305 INFO    allennlp.common.params: model.similarity_function.tensor_2_dim = 200\n",
            "2021-04-26 19:53:57.306 INFO    allennlp.common.params: model.similarity_function.combination = x,y,x*y\n",
            "2021-04-26 19:53:57.307 INFO    allennlp.common.params: model.similarity_function.activation = None\n",
            "2021-04-26 19:53:57.317 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 800, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.318 INFO    allennlp.common.params: model.modeling_layer.type = lstm\n",
            "2021-04-26 19:53:57.320 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper.LstmSeq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 800, 'num_layers': 2} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.321 INFO    allennlp.common.params: model.modeling_layer.input_size = 800\n",
            "2021-04-26 19:53:57.322 INFO    allennlp.common.params: model.modeling_layer.hidden_size = 100\n",
            "2021-04-26 19:53:57.323 INFO    allennlp.common.params: model.modeling_layer.num_layers = 2\n",
            "2021-04-26 19:53:57.324 INFO    allennlp.common.params: model.modeling_layer.bias = True\n",
            "2021-04-26 19:53:57.325 INFO    allennlp.common.params: model.modeling_layer.dropout = 0.2\n",
            "2021-04-26 19:53:57.327 INFO    allennlp.common.params: model.modeling_layer.bidirectional = True\n",
            "2021-04-26 19:53:57.328 INFO    allennlp.common.params: model.modeling_layer.stateful = False\n",
            "2021-04-26 19:53:57.353 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 1400, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.355 INFO    allennlp.common.params: model.span_end_encoder.type = lstm\n",
            "2021-04-26 19:53:57.356 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper.LstmSeq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.2, 'hidden_size': 100, 'input_size': 1400, 'num_layers': 1} and extras {'vocab'}\n",
            "2021-04-26 19:53:57.358 INFO    allennlp.common.params: model.span_end_encoder.input_size = 1400\n",
            "2021-04-26 19:53:57.359 INFO    allennlp.common.params: model.span_end_encoder.hidden_size = 100\n",
            "2021-04-26 19:53:57.360 INFO    allennlp.common.params: model.span_end_encoder.num_layers = 1\n",
            "2021-04-26 19:53:57.361 INFO    allennlp.common.params: model.span_end_encoder.bias = True\n",
            "2021-04-26 19:53:57.363 INFO    allennlp.common.params: model.span_end_encoder.dropout = 0.2\n",
            "2021-04-26 19:53:57.364 INFO    allennlp.common.params: model.span_end_encoder.bidirectional = True\n",
            "2021-04-26 19:53:57.365 INFO    allennlp.common.params: model.span_end_encoder.stateful = False\n",
            "2021-04-26 19:53:57.395 INFO    allennlp.common.params: model.dropout = 0.2\n",
            "2021-04-26 19:53:57.397 INFO    allennlp.common.params: model.mask_lstms = True\n",
            "2021-04-26 19:53:57.399 INFO    allennlp.common.params: model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7f45aecd1d90>\n",
            "2021-04-26 19:53:57.402 INFO    allennlp.common.params: model.regularizer = None\n",
            "2021-04-26 19:53:57.414 INFO    allennlp.nn.initializers: Initializing parameters\n",
            "2021-04-26 19:53:57.417 INFO    allennlp.nn.initializers: Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2021-04-26 19:53:57.421 INFO    allennlp.nn.initializers:    _highway_layer._module._layers.0.bias\n",
            "2021-04-26 19:53:57.426 INFO    allennlp.nn.initializers:    _highway_layer._module._layers.0.weight\n",
            "2021-04-26 19:53:57.429 INFO    allennlp.nn.initializers:    _highway_layer._module._layers.1.bias\n",
            "2021-04-26 19:53:57.435 INFO    allennlp.nn.initializers:    _highway_layer._module._layers.1.weight\n",
            "2021-04-26 19:53:57.442 INFO    allennlp.nn.initializers:    _matrix_attention._similarity_function._bias\n",
            "2021-04-26 19:53:57.447 INFO    allennlp.nn.initializers:    _matrix_attention._similarity_function._weight_vector\n",
            "2021-04-26 19:53:57.449 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_hh_l0\n",
            "2021-04-26 19:53:57.450 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_hh_l0_reverse\n",
            "2021-04-26 19:53:57.462 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_hh_l1\n",
            "2021-04-26 19:53:57.464 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_hh_l1_reverse\n",
            "2021-04-26 19:53:57.466 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_ih_l0\n",
            "2021-04-26 19:53:57.467 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_ih_l0_reverse\n",
            "2021-04-26 19:53:57.469 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_ih_l1\n",
            "2021-04-26 19:53:57.470 INFO    allennlp.nn.initializers:    _modeling_layer._module.bias_ih_l1_reverse\n",
            "2021-04-26 19:53:57.472 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_hh_l0\n",
            "2021-04-26 19:53:57.473 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_hh_l0_reverse\n",
            "2021-04-26 19:53:57.475 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_hh_l1\n",
            "2021-04-26 19:53:57.483 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_hh_l1_reverse\n",
            "2021-04-26 19:53:57.488 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_ih_l0\n",
            "2021-04-26 19:53:57.494 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_ih_l0_reverse\n",
            "2021-04-26 19:53:57.502 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_ih_l1\n",
            "2021-04-26 19:53:57.505 INFO    allennlp.nn.initializers:    _modeling_layer._module.weight_ih_l1_reverse\n",
            "2021-04-26 19:53:57.508 INFO    allennlp.nn.initializers:    _phrase_layer._module.bias_hh_l0\n",
            "2021-04-26 19:53:57.514 INFO    allennlp.nn.initializers:    _phrase_layer._module.bias_hh_l0_reverse\n",
            "2021-04-26 19:53:57.521 INFO    allennlp.nn.initializers:    _phrase_layer._module.bias_ih_l0\n",
            "2021-04-26 19:53:57.524 INFO    allennlp.nn.initializers:    _phrase_layer._module.bias_ih_l0_reverse\n",
            "2021-04-26 19:53:57.532 INFO    allennlp.nn.initializers:    _phrase_layer._module.weight_hh_l0\n",
            "2021-04-26 19:53:57.533 INFO    allennlp.nn.initializers:    _phrase_layer._module.weight_hh_l0_reverse\n",
            "2021-04-26 19:53:57.535 INFO    allennlp.nn.initializers:    _phrase_layer._module.weight_ih_l0\n",
            "2021-04-26 19:53:57.536 INFO    allennlp.nn.initializers:    _phrase_layer._module.weight_ih_l0_reverse\n",
            "2021-04-26 19:53:57.537 INFO    allennlp.nn.initializers:    _span_end_encoder._module.bias_hh_l0\n",
            "2021-04-26 19:53:57.538 INFO    allennlp.nn.initializers:    _span_end_encoder._module.bias_hh_l0_reverse\n",
            "2021-04-26 19:53:57.540 INFO    allennlp.nn.initializers:    _span_end_encoder._module.bias_ih_l0\n",
            "2021-04-26 19:53:57.544 INFO    allennlp.nn.initializers:    _span_end_encoder._module.bias_ih_l0_reverse\n",
            "2021-04-26 19:53:57.549 INFO    allennlp.nn.initializers:    _span_end_encoder._module.weight_hh_l0\n",
            "2021-04-26 19:53:57.551 INFO    allennlp.nn.initializers:    _span_end_encoder._module.weight_hh_l0_reverse\n",
            "2021-04-26 19:53:57.552 INFO    allennlp.nn.initializers:    _span_end_encoder._module.weight_ih_l0\n",
            "2021-04-26 19:53:57.553 INFO    allennlp.nn.initializers:    _span_end_encoder._module.weight_ih_l0_reverse\n",
            "2021-04-26 19:53:57.554 INFO    allennlp.nn.initializers:    _span_end_predictor._module.bias\n",
            "2021-04-26 19:53:57.557 INFO    allennlp.nn.initializers:    _span_end_predictor._module.weight\n",
            "2021-04-26 19:53:57.558 INFO    allennlp.nn.initializers:    _span_start_predictor._module.bias\n",
            "2021-04-26 19:53:57.560 INFO    allennlp.nn.initializers:    _span_start_predictor._module.weight\n",
            "2021-04-26 19:53:57.561 INFO    allennlp.nn.initializers:    _text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
            "2021-04-26 19:53:57.564 INFO    allennlp.nn.initializers:    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
            "2021-04-26 19:53:57.566 INFO    allennlp.nn.initializers:    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
            "2021-04-26 19:53:57.567 INFO    allennlp.nn.initializers:    _text_field_embedder.token_embedder_tokens.weight\n",
            "2021-04-26 19:53:57.570 INFO    root: Loading a model trained before embedding extension was implemented; pass an explicit vocab namespace if you want to extend the vocabulary.\n",
            "2021-04-26 19:53:57.706 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'token_indexers': {'token_characters': {'character_tokenizer': {'byte_encoding': 'utf-8', 'end_tokens': [260, 0, 0, 0, 0, 0], 'start_tokens': [259]}, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'squad'} and extras set()\n",
            "2021-04-26 19:53:57.710 INFO    allennlp.common.params: dataset_reader.type = squad\n",
            "2021-04-26 19:53:57.715 INFO    allennlp.common.from_params: instantiating class <class 'allennlp_rc.dataset_readers.squad.SquadReader'> from params {'token_indexers': {'token_characters': {'character_tokenizer': {'byte_encoding': 'utf-8', 'end_tokens': [260, 0, 0, 0, 0, 0], 'start_tokens': [259]}, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()\n",
            "2021-04-26 19:53:57.718 INFO    allennlp.common.params: dataset_reader.tokenizer = None\n",
            "2021-04-26 19:53:57.727 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'character_tokenizer': {'byte_encoding': 'utf-8', 'end_tokens': [260, 0, 0, 0, 0, 0], 'start_tokens': [259]}, 'type': 'characters'} and extras set()\n",
            "2021-04-26 19:53:57.729 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.type = characters\n",
            "2021-04-26 19:53:57.735 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer'> from params {'character_tokenizer': {'byte_encoding': 'utf-8', 'end_tokens': [260, 0, 0, 0, 0, 0], 'start_tokens': [259]}} and extras set()\n",
            "2021-04-26 19:53:57.738 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
            "2021-04-26 19:53:57.743 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer'> from params {'byte_encoding': 'utf-8', 'end_tokens': [260, 0, 0, 0, 0, 0], 'start_tokens': [259]} and extras set()\n",
            "2021-04-26 19:53:57.747 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.character_tokenizer.byte_encoding = utf-8\n",
            "2021-04-26 19:53:57.749 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.character_tokenizer.lowercase_characters = False\n",
            "2021-04-26 19:53:57.750 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.character_tokenizer.start_tokens = [259]\n",
            "2021-04-26 19:53:57.752 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.character_tokenizer.end_tokens = [260, 0, 0, 0, 0, 0]\n",
            "2021-04-26 19:53:57.753 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.start_tokens = None\n",
            "2021-04-26 19:53:57.754 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.end_tokens = None\n",
            "2021-04-26 19:53:57.756 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
            "2021-04-26 19:53:57.757 INFO    allennlp.common.params: dataset_reader.token_indexers.token_characters.token_min_padding_length = 0\n",
            "/usr/local/lib/python3.7/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:59: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
            "  UserWarning,\n",
            "2021-04-26 19:53:57.759 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.data.token_indexers.token_indexer.TokenIndexer'> from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()\n",
            "2021-04-26 19:53:57.760 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.type = single_id\n",
            "2021-04-26 19:53:57.761 INFO    allennlp.common.from_params: instantiating class <class 'allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer'> from params {'lowercase_tokens': True} and extras set()\n",
            "2021-04-26 19:53:57.762 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.namespace = tokens\n",
            "2021-04-26 19:53:57.764 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
            "2021-04-26 19:53:57.765 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.start_tokens = None\n",
            "2021-04-26 19:53:57.766 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.end_tokens = None\n",
            "2021-04-26 19:53:57.767 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.feature_name = text\n",
            "2021-04-26 19:53:57.769 INFO    allennlp.common.params: dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2021-04-26 19:53:57.770 INFO    allennlp.common.params: dataset_reader.lazy = False\n",
            "2021-04-26 19:53:57.771 INFO    allennlp.common.params: dataset_reader.passage_length_limit = None\n",
            "2021-04-26 19:53:57.772 INFO    allennlp.common.params: dataset_reader.question_length_limit = None\n",
            "2021-04-26 19:53:57.773 INFO    allennlp.common.params: dataset_reader.skip_invalid_examples = False\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hpWSPsmOvzNO",
        "outputId": "a7df99b2-fbb2-4705-a617-d8d9ba2ee171"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() \n",
        "filename = \"context.txt\"\n",
        "new_file = uploaded[filename].decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-394a9711-a24a-445e-aeaa-d6832d01d1a9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-394a9711-a24a-445e-aeaa-d6832d01d1a9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving context.txt to context.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmF-wDTT5oPJ",
        "outputId": "aa07b6ca-634c-4c42-dd13-c072c4a5ab59"
      },
      "source": [
        "print(new_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The purpose of SPARK is to help scientists find and better understand the potential causes of autism. As part of this effort, we study DNA from people with autism and from their family members who may or may not have autism. We also study information about their health and things that can impact health like behavior and lifestyle. To succeed, SPARK needs many thousands of people with autism and their families to join. What we collect and learn will be shared with many autism researchers to help speed up the progress of autism research. You should take part in Spark to help shape the future of autism research. Your TDNA could spark the next genetic discovery. With DNA from thousands of families across the country, we will be able to learn more about genes that may be related to autism and discover new ones. In return, you will be able to get updates on the latest research, join other autism research studies, find possible genetic causes of autism in your own family, and power future autism research for years to come. The people who can take part in SPARK if you or your dependent(s): Have a diagnosis of autism spectrum disorder (ASD). This can include Asperger syndrome, autism/autistic disorder, and pervasive developmental disorder not otherwise specified (PDD-NOS). Currently live in the U.S. or are serving in the U.S military abroad. Can read and understand English. All ages are welcome! At this time individuals who are eligible to register in SPARK include: The legal guardian of a dependent or dependents with a professional diagnosis of ASD. Guardians can register on behalf of themselves and their dependent(s). In relevant circumstances, legal guardians can invite biological parents of their dependent(s) to join if they are able/choose to. An independent adult with a professional diagnosis. Adults can register for themselves and can invite biological parents and full adult siblings to join as well. They can also register their own dependent(s), if applicable, during their registration. Please note: If you have an eligible diagnosis (see above), you can take part in SPARK even if your siblings or parents do not. If your child has autism the other family members who can join SPARK include:Both of the child(ren)âs birth parents. Any additional children with a formal ASD diagnosis. They must be under the age of 18 or your legally dependent adult child.One full biological sibling of the child with autism who does not have an ASD diagnosis can be added. However, parents who have more than one child with ASD may add all siblings without autism. If your child is adopted you can still take part in Spark. For more, see: Who can take part in SPARK?Ã¢â‚¬Â and If my child has autism, who else in the family can take part in SPARK? You can enroll in Spark if you have Yes. Families who have already completed clinical genetic testing are encouraged to participate in SPARK and complete our saliva collection process if they are otherwise eligible for our study. Participating in SPARK allows their de-identified information to become available to top researchers who are moving autism research forward and helping to find answers for the community at large. Participating also means that the family will have the opportunity to take part in additional research opportunities if they are interested. Such opportunities may become available through SPARKs research match program. You can read more about SPARKs research match program here. If earlier genetic testing for autism found nothing then you and your child can still take part in Spark. New genes and genetic changes related to autism are found on a regular basis, and one of SPARKs goals is to discover more of these. You can stop taking part in SPARK at any time. Your familyâ€™s privacy is a top priority of our study, and participation in SPARK is completely voluntary. You need only send an email to info@SPARKforAutism.org if you would like to withdraw. Spark needs to study so many people since the more people who enroll in SPARK, the more genes and genetic changes related to autism we are likely to discover. When we compare the DNA of thousands of people, we can find differences between individuals. Sometimes these differences are related to autism, and other times they are not. SPARK aims to enroll 50,000 families affected by autism in order to move autism research forward more quickly. Spark will share its findings.SPARK data will be shared with the entire autism research community (doctors, experts and researchers) so that they can study and learn from it. We are also publishing findings in scientific journals and returning information to our participants via study reports. SPARK is sponsored by the Simons Foundation Autism Research Initiative (SFARI). SFARIâ€™s mission is to improve the understanding, diagnosis and treatment of autism spectrum disorder (ASD) by funding cutting-edge research of the highest quality and significance. For more information on SFARI, please visit https://sfari.org/. SPARK clinical sites are a network of 25 autism research centers at universities and hospitals throughout the country that have partnered with SPARK. These sites help educate their local communities about SPARK, enroll new participants, and run autism research studies of their own. Click here to learn more and to see if thereâ€™s a SPARK clinical site near you! In order to join Spark you have to enroll online at www.SPARKforAutism.org. It will involve several steps. Each independent adult participant must have their own email address. You can start and stop the registration process at any time, and your information will be saved along the way. You also have the option of completing your registration at one of our 25 clinical sites across the country. Click here to see if thereâ€™s a site near you!Also, when you enrolled in SPARK, we asked if we could contact you about other autism research that may be relevant to you or your family. If you agreed, you may be invited to join these other studies via email. Participation in these studies is completely voluntary. There is no cost to join SPARK. You will never be asked to give money as part of this project.The time commitment for enrolling online takes between 15 and 30 minutes. You can start and stop the process at any time, and your information will be saved along the way. For the saliva sample collection, producing enough saliva can take anywhere from 15 minutes to an hour. If you forgot your login information then: your username is the email address you used when you registered with SPARK. If you have forgotten your password, you can reset it by clicking here or selecting Login on the SPARKforAutism.org home page. On the login page, click on Ã¢â‚¬Å“Reset Password.To invite other people during registration, you may be prompted to invite biological relatives like parents or siblings via email. You can also invite other family members to participate after youve registered by using the invitation tool on your dashboard. We will let you know when other invitation features become available. Please note: We know that each family in our community is different from the next, and we do not expect anyone to invite family members or relatives that they know are either unable to or uninterested in participating. If you invited someone to participate in SPARK and they haven't joined yet, please double-check to make sure you used the correct email address during the invitation process and ask them to check that the invitation did not go into a spam folder. If there is a problem, you can resend an invitation from your study dashboard. The SPARK study team will send up to three reminder messages to individuals who were invited to join SPARK by another participant. After that, the SPARK study team will make no further attempts to reach that individual.If you or your child are unable to spit, you can use the sponges that are included with each saliva kit. Caregivers can collect the sample by sponging the inside of the persons cheek and pressing the sponge against the notch of the funnel so that the saliva will flow into the tube. You can find video resources that will further explain the saliva collection process here.The surveys that appear on the participant dash board is to collect medical and behavioral information. The number and types of surveys that you receive on your study dashboard will vary from person to person. The number and types of surveys you will be asked to complete depend on a number of factors, such as whether or not the individual has an ASD diagnosis and the individuals age. None of the surveys on your study dashboard are required, but we are grateful when you complete them. They will help us learn much more. Only the primary account holders and/or invited family members with an ASD diagnosis will receive surveys. For example, if you invite your child's biological parent to join, they will not be asked to complete any surveys on themselves or their dependents unless they have an ASD diagnosis.The surveys help us collect information about: Other medical conditions. Behavior. Social and physical development. Any services received. Family medical history. Spark will tell you about results from surveys you fill out. While the results do not represent a clinical evaluation, it is our hope that participants find value in survey results. For example, if a family's survey results show that an individual has a lot of concerns about a family member, the individual could talk to their doctor, school district, etc., and perhaps even use the results as a starting place for that conversation. By joining SPARK, you will have the chance to learn about and participate in other studies led by autism researchers from around the world. We call this program research match. For these studies, you will only be contacted if you or your family meet the enrollment criteria. You do not have to take part in any of these studies. It is your choice. If you decide not to join one study, it will not prevent you from joining others. Studies may take place online, over the phone, or in person. They range in topics from genetics to behavior interventions. Our genes contain the instructions, or code, that tell our cells how to grow, develop and work. “Genetic difference” refers to a change in a gene. These differences can range from being harmful to helpful. Some can have no effect. Once our lab receives a family’s samples, we look for genetic differences that are definitively linked to autism. If such changes are found and confirmed, the family is contacted about those results. SPARK will return the genetic result either through a SPARK genetic counselor or through the family’s own medical provider.\n",
            "Since new autism genes are discovered every year, each family’s genes will be re-analyzed every year and rechecked for results. Not everyone in Spark will receive genetic results. Because SPARK is a research study, our genetic analysis is not like a clinical genetic test or commercial sequencing service. SPARK provides genetic results in the form of a clinical report only if we discover a genetic change associated with autism.Not everyone in SPARK will have genetic findings linked to autism. Based on what we know today about genes that are linked to autism, SPARK scientists expect to find a genetic difference linked to autism in 5% to 10% of people in the study. This number will increase as we learn more about autism and identify more genes that are linked to autism.There is also a chance that we will find genes linked to autism in your sample only after we have studied genes from many other families and compared them with yours. In this case, it could be years before there are results to return. Keep in mind that genes are not the only cause of autism, so not all people with autism will have a genetic difference. SPARK can collect an individual’s genetic information from a small sample of saliva. We’ll mail a saliva collection kit to your home. You can either spit directly into the tube or use the sponges provided in the kit to collect saliva.\n",
            "We know the process of spitting can be unpleasant or hard for many people with autism. Our hope is that giving saliva will be simpler than giving blood. We are working on providing alternative methods to collect DNA, and we will contact the relevant participants when these become available.Once a saliva sample arrives at the lab, it goes through processing. During this stage, the lab makes sure the sample can be used. For example, they check to see if there is enough DNA in the saliva for it to be usable. After processing, the sample is stored until we send it out for sequencing. We do not sequence samples as they come in. Instead, for logistical reasons, we wait until we have large batches of samples ready. Storage does not affect the integrity of the DNA. During sequencing, we look for genetic differences that are definitively linked to autism. If such changes are found and confirmed, the family is contacted regarding those results. It can take many months to study genes and confirm findings. If we find genes linked to autism in your sample, it will be at least one year from the date we get your sample before the results will be ready to share. When SPARK has a genetic result to return to a family, the primary account holder is contacted by email. The family can choose if they would like to receive the result through a SPARK genetic counselor or through their own medical provider. For more, see “Can having genes linked to autism affect my health insurance coverage?” Spark will very rarely tell us about genetic differences not related to autism. In almost all cases, the genetic results we return will be related to autism. If we do return a result unrelated to autism, we will do so based on current recommendations from the American College of Medical Genetics. Studying genes has shown that genetic differences play a big role in autism. There are likely hundreds of genes involved in autism, and while some of these genes are already known, very large studies like SPARK can help find others.Learning about the genetic causes of autism will help us find potential treatments and preventive measures for things that are common within subtypes of autism, like digestive issues and seizures. Since everyone has different genes, some treatments may work well for one person while other treatments may work better for someone else. In the future, treatment will likely be tailored based on your or your child’s particular causes of autism. If there is no history of autism in your family, then your Childs autism could come from different types of genetic changes can contribute to autism. In some cases, genetic changes are passed down (inherited) from parents to their children. In other cases, a random change takes place in the sperm or egg because the process of copying DNA is not perfect. This change to the genetic code is considered a “de novo” (new) change. Studying genes can help us find changes linked to autism no matter when they take place. Knowing that you or your child has genes linked to autism may be of help:When there is an opportunity to take part in research or clinical trials matching your results.When you want to connect with other people or families who share the same diagnosis.If you or other family members want to know if you or they have a higher likelihood of having a future child with autism. We need saliva from multiple family members as we are more likely to find clues about a person’s autism when we can also study saliva samples from their biological parents and full biological brothers or sisters. Family members share many genes. Comparing samples from family members makes it easier for us to spot differences in genes.Under the federal Genetic Information Nondiscrimination Act (GINA), having genes linked to autism should not affect your existing health coverage or whether you qualify for health coverage. To learn more, please visit the GINA website here. Yes when studying genes we can tell from saliva samples whether people are related. However, if we find that someone is not the biological parent, we will not share this information with you or anyone else. You can find copies of your signed consent forms on your study dashboard under “My Documents.” If you would like to change your consents or assents, please contact us at info@SPARKforAutism.org. Assent is a child’s or dependent’s agreement that he or she is willing to participate in research. Assent is used with people who are dependent or are too young to give informed consent. Assent is needed if they are able to understand the research, its expected risks and possible benefits, and the activities expected of them as subjects. To receive assent from your child or dependent, you must explain this information and ask if he or she agrees. If the child gives assent, informed consent must also be obtained from the person’s parent or guardian. To participate in SPARK, children ages 10 through 17 and dependent adults are asked to assent, if they are capable. Yes you can still participate in SPARK and contribute behavioral data without consenting to submit saliva and share your genetic data with researchers. No. If you do not consent to share your genetic data with researchers, we will not send a saliva collection kit to you. Therefore, we will not have a saliva sample to analyze in order to send you results. SPARK is very thankful for our active community of participants! To express our gratitude, we issue Amazon.com Gift Card codes to participants who complete certain tasks.Your Amazon.com Gift Card code will be sent to you in the form of an alphanumeric code to use toward purchases on Amazon.com and will no longer appear on your dashboard. You will receive this code via email.Saliva Donation: Within a few weeks of receiving adequate saliva samples from a family, the primary account holder will be issued up to $50 in Amazon.com Gift Card codes. The $50 is issued per family (including invited participants), rather than per participant. $50 is the maximum amount of gift codes a participating family is eligible to receive in recognition of saliva sample donation.Survey Rounds:Within a few weeks of completing all Round 2 surveys, the primary account holder will be issued a $25 Amazon.com Gift Card code. Other Study Opportunities: If you sign up for more research studies in the future, such as through our research matching program, there may be additional compensation. Amazon.com Gift Card codes for participation in SPARK are provided per family rather than per individual enrolled. They are emailed to the primary account holder. Your Amazon.com Gift Card code will arrive via email within a few weeks of your completing certain activities. We appreciate your patience! Saliva Donation: It may take several weeks for us to receive your kits and confirm that the samples can be analyzed. Your gift code will be issued within a few weeks of confirming receipt of an adequate saliva sample or samples. You can track the progress of your sample(s) on your dashboard. Survey Rounds: Round 2 survey gift codes are issued and emailed within a few weeks of completion of all assigned surveys. If it has been a few weeks, and my Amazon.com Gift Card code still has not arrived then please first check your SPARK dashboard for any updates on the status of your Amazon.com Gift Card code. Please email info@SPARKforAutism.org if you have additional questions. If you received a $25 Amazon.com Gift Card code instead of a $50 code for your saliva sample(s), it is because someone you invited to participate did not return a complete saliva sample. If you have further questions about this, please review the data consent form found in the “My Documents” section of your dashboard or contact info@SPARKforAutism.org. To redeem the Amazon.com Gift Card Code you must first have an Amazon account. If you do not have an account with Amazon, you can set one up here. Once you are logged in to Amazon, you can redeem your code by clicking on “Accounts and Lists” then “Your Account” near the upper right corner of the page. You will then be directed to a new page where you will click on “Gift Cards”. A new page will open where you can redeem a gift card. After you click on “Redeem a Gift Card”, you will be directed to a new page. On this page you will enter the alphanumeric 14-character code that you received in the email from SPARK. Click “Apply to your balance”. You will now have the funds available to make any purchase on Amazon. I am having trouble with my Amazon.com Gift Card code then Amazon has a helpful page with tips for resolving gift card redemption problems. Spark uses saliva to collect data as saliva samples offer a convenient method for DNA collection as compared with other sources of DNA, such as a cheek swab or blood samples. Saliva collection is a common method for research ranging from large scientific studies to individual personalized medicine tests. We need saliva from beyond child or adult with ASD as we are more likely to discover information about autism if we can compare an individual’s DNA with the DNA of their biological relatives. Saliva kits are shipped to each address that you provide during registration. As a reminder, kits can be shipped only within the U.S. Individuals who are stationed overseas in the military should use their current FPO or APO address. Yes you can give multiple shipping addresses for different participants. We will ship saliva kits to multiple addresses you give us at registration. If your saliva kit has not arrived then, after you enroll in SPARK, it will take two to three weeks for the kit(s) to be delivered to the address(es) you provided. Please periodically check your SPARK dashboard for any updates during that time. If you notice an error, please double-check that your address is given correctly. If you need to make changes to your address, you can do so by visiting your SPARK dashboard. If you have not received your kit after three weeks, please contact info@SPARKforAutism.org. If your saliva collection kit is damaged or items are missing, please contact info@SPARKforAutism.org. We only ship saliva kits to addresses within the United States. Individuals who are stationed overseas in the military should use their current FPO or APO address. If you received the saliva kit in the mail to use the kit and provide saliva samples you follow the directions in the saliva collection box will take you step by step through the collection process. You and/or your child should not eat or drink for 30 minutes before spitting. Tips for parents: Hold the tube to your child’s mouth, or if your child is able, allow them to hold the tube and ask them to spit into it. If your child holds the tube, don’t let the saliva spill. You must get enough saliva, without air bubbles, to fill the tube to the line that is marked on the tube. This will likely require spitting multiple times. If you are having a hard time, we suggest giving yourself or your child a break for a few minutes in between spitting. Then spit or ask your child to spit into the tube again. Feel free to continue taking breaks and having them try again until you have collected enough saliva to get up to the line. During these breaks, it is important to not close the tube, so that the preserving liquid does not release and mix with the saliva just yet.We know the saliva collection can be challenging, but we appreciate your patience and continued efforts. For more detailed information on the saliva collection kit instructions—including what to do if you cannot get your child to spit into the tube—please email info@SPARKforAutism.org. Completing the saliva collection can be difficult for many people. You will find a helpful instructional video to walk you through the saliva collection process. The instructions that came in the box also include a section for if you and/or your child cannot spit. Additionally, if you and/or your child(ren) are unable to collect saliva by spitting or using the sponge – we are working on providing alternative collection methods. These would include blood draws and cheek swabs, and we will contact participants when these become available. After completing saliva collection, please return your sample in the postage-paid box the kit came in. You can drop it in the mail or hand it to a USPS mail delivery person. The postage-paid return label is already attached to the bottom of the box. When you send your saliva sample back, We will keep your saliva sample in a secure laboratory, and if it passes quality control, we will extract DNA from it. Research staff may separate the samples into smaller amounts and freeze them, so they will be available for research for an indefinite period of time. The samples will be stripped of your personal identifying information and labeled with a unique study identification number. Spark is protecting my data as: All research data that could identify you or your child is replaced with a study code called a global unique identifier (GUID). GUIDs allow researchers to share data about you without sharing anything that could identify you. All your research data and records are stored electronically in a secure, encrypted, password-protected database. We will not release research data about you or your child to others, unless required by law. We will never publish anything about the study on any forum that could identify you without your express permission. Our third-party service providers and consultants are legally required to keep all your research data private. While we cannot guarantee total privacy, we make every effort to maintain your privacy. Your research data will never be shared with third parties without your consent. When you join SPARK, you allow the Simons Foundation Autism Research Initiative (SFARI) to share your research data with the National Database for Autism Research (NDAR). NDAR is a central research data source for autism researchers from around the country. Data in NDAR is stored without any information that could identify you. Before any of your research data from SPARK is shared for other autism research, all information that could identify you is replaced with a global unique identifier (GUID). GUIDs allow researchers to share research data about you without exposing anything that could identify you. To learn more about GUIDs, contact info@SPARKforAutism.org. On the date you withdraw from SPARK, we will stop asking for your research data and sharing it with other studies. Any data that has already been shared with or used by other studies cannot be taken back. If you would like to withdraw from SPARK, please send an email to info@SPARKforAutism.org. SPARK collects two main types of data: information that participants share through surveys and the DNA data that comes from participants’ saliva samples. Both types of information are stored securely and are encrypted, for an extra layer of protection.Identifying information like names, phone numbers or email addresses is not shared with anyone outside the research study. Identifying information is only shared with researchers who are using SPARK data for research. You are in control of the information you share. We will not share your data without your permission. If you choose to participate in a study, we will share your contact information with researchers so that they can contact you. After your DNA has been sequenced, we will keep your saliva sample in a secure laboratory. If we have received enough DNA, research staff may separate the samples into smaller amounts and freeze them, so they will be available for research for an indefinite period of time. The samples will be stripped of your personal identifying information and labeled with a unique study identification number. DNA sequencing decodes the genetic letters that make up an individual’s genome. SPARK only looks at the portion of DNA that codes for proteins, because most genetic differences related to autism are found in this part of an individual’s DNA. The information generated from SPARK’s sequencing analysis cannot be used to determine a person’s identity. All sequencing data is stored securely in the SPARK database. Researchers who request access to SPARK’s genetic data must also agree to store this information securely and may only use it for scientific purposes that they designate in their request. From the DNA that is provided to Spark, the genetic information we generate is stored securely in the SPARK database. SPARK and affiliated scientists analyze this information to better understand the genetic components of autism. Sometimes the results of this research will appear in scientific journals. The information that appears in these journals cannot be used to expose an individual’s identity. SPARK study staff and researchers from around the world depend on DNA from Spark. Outside researchers must go through an approval process before they are allowed to access SPARK’s DNA database. It is possible that the vendors we work with (for example, a company that hosts a SPARK survey) could identify participants from their data. However, all of our vendors sign privacy agreements and are required by law to abide by these agreements. Your contact information will not be shared with researchers through SPARK’s research matching program unless you give us permission. SPARK does not and will never sell data or DNA to anyone. All information shared with SPARK is kept securely in its database. Only if you request and receive a genetic result from SPARK will this information go into your medical record. Only with your permission will the doctor have access to information that you share with Spark. The only doctor who will receive information from SPARK will be the one you designate to receive the genetic result should a genetic result be available. The federal Genetic Information Nondiscrimination Act (GINA) makes it illegal for employers, health insurers and group health plans to discriminate against individuals based on their genetic information. However, if you do receive a genetic result through SPARK, the physician you designated will put this information into your medical record, and there may be insurance implications. No other family members can see information that you enter as a SPARK participant. All participants over the age of 18 are required to have their own accounts. Spark verifies participants identify by requiring that each individual who registers has a unique email address. The police or FBI does not have access to DNA or other information collected. To protect your privacy, we have obtained a Certificate of Confidentiality from the National Institutes of Mental Health. Researchers can use this certificate to legally refuse to disclose information that may identify you in any federal, state or local civil, criminal, administrative, legislative or other proceedings—for example, if there is a court subpoena. Researchers will use the certificate to resist any demands for information that would identify you, except as explained below: The certificate cannot be used to resist a demand for information from personnel of the United States federal or state government agency sponsoring the project and that will be used for auditing or program evaluation of agency-funded projects or for information that must be disclosed in order to meet the requirements of the federal Food and Drug Administration. The certificate does not prevent you or a member of your family from voluntarily releasing information about your child, yourself or your involvement in this research. If an insurer or employer learns about you and/or your child’s participation and obtains your consent to receive research information, then the investigator may not use the Certificate of Confidentiality to withhold this information.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg1aAEtEyfp2",
        "outputId": "884d90e2-b6c1-4ca1-84ee-17f6fcefecb1"
      },
      "source": [
        "passage = st.text_area(\"Article\",new_file)\n",
        "question = st.text_input(\"Question\", \"Would police or the FBI ever be able to access DNA or other information collected?\")\n",
        "result = model.predict(question, passage)\n",
        "start, end = result[\"best_span\"]\n",
        "question_tokens = result[\"question_tokens\"]\n",
        "passage_tokens = result[\"passage_tokens\"]\n",
        "# print(start)\n",
        "# print(end)\n",
        "# print(passage_tokens)\n",
        "mds = [f\"*{token}\" if start <= i <= end else token if start - 10 <= i <= end + 10 else \"\" for i, token in enumerate(passage_tokens)]\n",
        "subs = '*'\n",
        "res = [i for i in mds if subs in i]\n",
        "print(res)\n",
        "print(' '.join(res).replace('*',''))\n",
        "# print(mds)\n",
        "# print(type(mds))\n",
        "# print(st.markdown(\" \".join(mds)))\n",
        "# attention = result[\"passage_question_attention\"]\n",
        "# print(attention)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['*does', '*not']\n",
            "does not\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}